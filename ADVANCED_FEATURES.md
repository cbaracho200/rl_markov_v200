# üéì Agente RL Avan√ßado - N√≠vel PhD

## Vis√£o Geral

Este framework implementa um agente de Reinforcement Learning de **n√≠vel PhD** com t√©cnicas state-of-the-art para previs√£o de s√©ries temporais econ√¥micas. O agente √© baseado em **Proximal Policy Optimization (PPO)** com m√∫ltiplas melhorias arquiteturais e algor√≠tmicas.

---

## üöÄ T√©cnicas Implementadas

### 1. **Transformer-based Actor-Critic**
- **Descri√ß√£o**: Arquitetura baseada em Transformer substituindo redes feedforward tradicionais
- **Benef√≠cios**:
  - Captura depend√™ncias de longo alcance em s√©ries temporais
  - Processamento paralelo eficiente
  - Melhor representa√ß√£o de padr√µes complexos
- **Implementa√ß√£o**: `TransformerActorCritic` em `rl_agent_advanced.py`

### 2. **Multi-Head Self-Attention**
- **Descri√ß√£o**: Mecanismo de aten√ß√£o com m√∫ltiplas cabe√ßas para capturar diferentes aspectos dos dados
- **Par√¢metros**: 8 cabe√ßas de aten√ß√£o por padr√£o
- **Benef√≠cios**:
  - Foca em diferentes partes da sequ√™ncia temporal simultaneamente
  - Aprende representa√ß√µes hier√°rquicas
  - Melhora interpretabilidade

### 3. **Prioritized Experience Replay (PER)**
- **Descri√ß√£o**: Buffer de replay que prioriza transi√ß√µes com maior TD-error
- **Estrutura**: Sum Tree para sampling eficiente O(log n)
- **Benef√≠cios**:
  - Aprende mais r√°pido com experi√™ncias "surpresa"
  - Uso mais eficiente de dados
  - Converg√™ncia mais est√°vel
- **Par√¢metros**:
  - Œ± = 0.6 (prioriza√ß√£o)
  - Œ≤ = 0.4 ‚Üí 1.0 (importance sampling)

### 4. **Noisy Networks**
- **Descri√ß√£o**: Adiciona ru√≠do parametrizado aos pesos da rede
- **Benef√≠cios**:
  - Explora√ß√£o adaptativa sem epsilon-greedy
  - Ru√≠do diminui naturalmente com o treinamento
  - Mais eficiente que explora√ß√£o uniforme
- **Implementa√ß√£o**: `NoisyLinear` substituindo `nn.Linear`

### 5. **Dueling Architecture**
- **Descri√ß√£o**: Separa fun√ß√£o valor em advantage e value streams
- **F√≥rmula**: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,¬∑)))`
- **Benef√≠cios**:
  - Aprende melhor quais estados s√£o valiosos
  - Melhora generaliza√ß√£o
  - Converg√™ncia mais r√°pida

### 6. **LSTM Memory**
- **Descri√ß√£o**: Camadas recorrentes para mem√≥ria de longo prazo
- **Configura√ß√£o**: 2 camadas LSTM ap√≥s Transformer
- **Benef√≠cios**:
  - Mant√©m contexto temporal entre steps
  - Captura depend√™ncias de prazo muito longo
  - Complementa aten√ß√£o do Transformer

### 7. **Ensemble de Critics (3 Critics)**
- **Descri√ß√£o**: Tr√™s redes critic independentes; usa m√©dia das previs√µes
- **Benef√≠cios**:
  - Reduz vi√©s de estimativa
  - Maior robustez
  - Menor vari√¢ncia nas estimativas de valor

### 8. **Adaptive Entropy Regularization**
- **Descri√ß√£o**: Coeficiente de entropia ajust√°vel automaticamente
- **M√©todo**: Otimiza√ß√£o dual com target entropy
- **Benef√≠cios**:
  - Balanceamento autom√°tico explora√ß√£o/explora√ß√£o
  - Adapta-se √† fase do treinamento
  - N√£o requer tuning manual

### 9. **Learning Rate Scheduling com Warmup**
- **Estrat√©gia**: Cosine Annealing with Warm Restarts
- **Configura√ß√£o**:
  - Warmup: 1000 steps
  - T_0 = 10, T_mult = 2
  - eta_min = lr * 0.1
- **Benef√≠cios**:
  - Converg√™ncia mais est√°vel
  - Evita m√≠nimos locais
  - Melhora generaliza√ß√£o

### 10. **Gradient Accumulation**
- **Descri√ß√£o**: Suporta mini-batches para simular batches grandes
- **Benef√≠cios**:
  - Treina com batches grandes em hardware limitado
  - Estimativas de gradiente mais est√°veis

### 11. **Spectral Normalization**
- **Descri√ß√£o**: Normaliza pesos para controlar Lipschitz constant
- **Benef√≠cios**:
  - Estabilidade no treinamento
  - Previne explos√£o/desaparecimento de gradientes

### 12. **Curriculum Learning**
- **Descri√ß√£o**: Aumenta dificuldade progressivamente
- **Est√°gios**:
  - Easy (0-30%): Padr√µes simples
  - Medium (30-60%): Complexidade moderada
  - Hard (60-100%): Cen√°rios completos
- **Benef√≠cios**:
  - Aprendizado mais eficiente
  - Menos falhas catastr√≥ficas

### 13. **Early Stopping**
- **Descri√ß√£o**: Para treinamento se n√£o h√° melhora
- **Patience**: 50 avalia√ß√µes
- **Benef√≠cios**:
  - Previne overfitting
  - Economiza tempo computacional

### 14. **Value Function Clipping**
- **Descri√ß√£o**: Clipa atualiza√ß√µes do critic como no actor
- **Benef√≠cios**:
  - Atualiza√ß√µes mais conservadoras
  - Maior estabilidade

### 15. **Gradient Clipping**
- **M√©todo**: Clip por norma (max_norm = 0.5)
- **Benef√≠cios**:
  - Previne explos√£o de gradientes
  - Treinamento mais est√°vel

---

## üìä Compara√ß√£o: Standard vs Advanced

| Caracter√≠stica | Standard RL | Advanced RL (PhD) |
|---|---|---|
| Arquitetura | Feedforward | Transformer + LSTM |
| Aten√ß√£o | Nenhuma | Multi-Head (8 heads) |
| Replay Buffer | FIFO simples | Prioritized (PER) |
| Explora√ß√£o | Ru√≠do gaussiano | Noisy Networks |
| Critics | 1 critic | Ensemble de 3 |
| Entropy Coef | Fixo | Adaptativo |
| Learning Rate | Fixo | Scheduling + Warmup |
| Curriculum | N√£o | Sim (3 est√°gios) |
| Early Stop | N√£o | Sim (patience=50) |
| Par√¢metros | ~50K | ~500K |

---

## üéØ Quando Usar Cada Vers√£o

### Use **Standard RL** (`RLAgent`) quando:
- ‚úÖ Dados limitados (< 100 pontos)
- ‚úÖ Hardware limitado (CPU b√°sico)
- ‚úÖ Prototipagem r√°pida
- ‚úÖ S√©ries simples com poucos padr√µes

### Use **Advanced RL** (`AdvancedRLAgent`) quando:
- ‚úÖ Dados abundantes (> 200 pontos)
- ‚úÖ S√©ries complexas com m√∫ltiplos padr√µes
- ‚úÖ GPU dispon√≠vel (recomendado)
- ‚úÖ M√°xima precis√£o necess√°ria
- ‚úÖ Produ√ß√£o / Research

---

## üíª Como Usar

### Exemplo B√°sico

```python
from src.agents import AdvancedRLAgent
from src.training import AdvancedRLTrainer
from src.environments import TimeSeriesEnv

# 1. Cria ambiente
env = TimeSeriesEnv(data=your_data, forecast_horizon=12)

# 2. Cria agente avan√ßado
agent = AdvancedRLAgent(
    state_dim=env.observation_space.shape[0],
    action_dim=env.action_space.shape[0],
    hidden_dim=512,
    num_heads=8,
    num_layers=3,
    use_per=True,
    use_noisy=True,
    use_lstm=True,
    device='cuda'  # Ou 'cpu'
)

# 3. Cria trainer
trainer = AdvancedRLTrainer(
    env=env,
    agent=agent,
    use_curriculum=True
)

# 4. Treina
history = trainer.train(
    n_episodes=500,
    early_stopping=True
)

# 5. Avalia
results = trainer.evaluate(n_episodes=10)
```

### Exemplo Completo

Veja `examples/advanced_rl_example.py` para exemplo completo com:
- Gera√ß√£o de dados sint√©ticos
- Configura√ß√£o detalhada
- Visualiza√ß√µes
- M√©tricas avan√ßadas

---

## üîß Hiperpar√¢metros Recomendados

### Para GPU (RTX 3090+)
```python
agent = AdvancedRLAgent(
    hidden_dim=512,
    num_heads=8,
    num_layers=3,
    buffer_size=100000,
    learning_rate=1e-4,
    device='cuda'
)

trainer.train(
    n_episodes=1000,
    batch_size=128
)
```

### Para CPU / Google Colab Free
```python
agent = AdvancedRLAgent(
    hidden_dim=256,
    num_heads=4,
    num_layers=2,
    buffer_size=50000,
    learning_rate=3e-4,
    device='cpu'
)

trainer.train(
    n_episodes=200,
    batch_size=64
)
```

---

## üìà M√©tricas de Treinamento

O trainer avan√ßado rastreia:
- **Episode Rewards**: Recompensa por epis√≥dio
- **Policy Loss**: Perda do actor
- **Value Loss**: Perda do critic
- **Entropy**: N√≠vel de explora√ß√£o
- **Entropy Coefficient**: Coef adaptativo
- **Learning Rate**: LR ao longo do tempo
- **Gradient Norms**: Para debug
- **Buffer Size**: Tamanho do replay buffer

---

## üéì Refer√™ncias Acad√™micas

1. **PPO**: Schulman et al. (2017) - "Proximal Policy Optimization Algorithms"
2. **Transformers**: Vaswani et al. (2017) - "Attention Is All You Need"
3. **PER**: Schaul et al. (2016) - "Prioritized Experience Replay"
4. **Noisy Nets**: Fortunato et al. (2018) - "Noisy Networks for Exploration"
5. **Dueling**: Wang et al. (2016) - "Dueling Network Architectures"
6. **GAE**: Schulman et al. (2016) - "High-Dimensional Continuous Control"
7. **SAC (Entropy)**: Haarnoja et al. (2018) - "Soft Actor-Critic"

---

## üêõ Debugging

### Problema: Loss n√£o converge
**Solu√ß√£o**: Reduza learning rate, aumente warmup steps

### Problema: Explora√ß√£o excessiva
**Solu√ß√£o**: Ajuste target_entropy para valor mais negativo

### Problema: OOM (Out of Memory)
**Solu√ß√£o**: Reduza hidden_dim, num_layers, ou batch_size

### Problema: Treinamento muito lento
**Solu√ß√£o**: Use GPU, reduza buffer_size, ou use agente standard

---

## üìû Suporte

Para quest√µes sobre implementa√ß√£o ou bugs, abra uma issue no reposit√≥rio.

---

## üìÑ Licen√ßa

Este c√≥digo √© fornecido para fins educacionais e de pesquisa.
