# üéì Modelos Avan√ßados & Otimiza√ß√£o Recursiva

## Vis√£o Geral

Este framework agora inclui **4 modelos supervisionados state-of-the-art** e **otimiza√ß√£o autom√°tica de hiperpar√¢metros com Optuna**.

---

## ü§ñ Novos Modelos Supervisionados

### 1. **AutoARIMA** (`AutoARIMAPredictor`)

**Descri√ß√£o**: ARIMA com busca autom√°tica de hiperpar√¢metros usando pmdarima.

**Vantagens**:
- ‚úÖ Encontra automaticamente os melhores par√¢metros (p, d, q)
- ‚úÖ Suporta sazonalidade autom√°tica (SARIMA)
- ‚úÖ Usa testes estat√≠sticos (ADF, KPSS)
- ‚úÖ Mais robusto que ARIMA manual

**Uso**:
```python
from src.models import AutoARIMAPredictor

model = AutoARIMAPredictor(
    max_p=5,              # M√°ximo AR order
    max_d=2,              # M√°ximo differencing
    max_q=5,              # M√°ximo MA order
    seasonal=True,        # Usa SARIMA
    m=12,                 # Per√≠odo sazonal (12 para mensal)
    stepwise=True,        # Busca stepwise (mais r√°pido)
    information_criterion='aic',  # 'aic', 'bic', ou 'hqic'
    trace=False           # Mostra progresso
)

model.fit(train_data)
predictions = model.predict(steps=12)

# Acessa melhores par√¢metros
best_params = model.get_best_parameters()
print(f"Melhor ordem: {best_params['order']}")
print(f"AIC: {best_params['aic']}")
```

**Quando usar**:
- S√©ries com padr√µes lineares e sazonalidade clara
- Quando n√£o sabe os melhores par√¢metros ARIMA
- Dados com tend√™ncia e sazonalidade

**Performance**: MAPE t√≠pico: 5-15%

---

### 2. **Prophet** (`ProphetPredictor`)

**Descri√ß√£o**: Modelo do Facebook robusto a outliers e dados faltantes.

**Vantagens**:
- ‚úÖ Robusto a dados faltantes e outliers
- ‚úÖ M√∫ltiplas sazonalidades (di√°ria, semanal, anual)
- ‚úÖ Detecta mudan√ßas de tend√™ncia automaticamente
- ‚úÖ Suporta feriados e eventos especiais
- ‚úÖ Interpret√°vel (componentes separados)

**Uso**:
```python
from src.models import ProphetPredictor

model = ProphetPredictor(
    seasonality_mode='multiplicative',  # ou 'additive'
    yearly_seasonality='auto',          # ou True/False
    weekly_seasonality='auto',
    daily_seasonality='auto',
    changepoint_prior_scale=0.05,       # Flexibilidade da tend√™ncia (0.001-0.5)
    seasonality_prior_scale=10.0        # For√ßa da sazonalidade (0.01-10)
)

model.fit(train_data)
predictions = model.predict(steps=12)
```

**Quando usar**:
- S√©ries com forte sazonalidade
- Dados com outliers ou valores faltantes
- M√∫ltiplas sazonalidades (ex: vendas com padr√µes semanais e anuais)
- Quando precisa de interpretabilidade

**Performance**: MAPE t√≠pico: 5-12%

**Hiperpar√¢metros chave**:
- `changepoint_prior_scale`: ‚Üë = mais flex√≠vel, ‚Üì = mais suave
- `seasonality_mode`: 'multiplicative' para s√©ries com sazonalidade crescente

---

### 3. **CatBoost** (`CatBoostPredictor`)

**Descri√ß√£o**: Gradient boosting state-of-the-art da Yandex.

**Vantagens**:
- ‚úÖ Melhor performance que XGBoost em muitos casos
- ‚úÖ Suporte nativo a features categ√≥ricas
- ‚úÖ Menor overfitting
- ‚úÖ Treinamento mais r√°pido
- ‚úÖ N√£o requer normaliza√ß√£o de dados

**Uso**:
```python
from src.models import CatBoostPredictor

model = CatBoostPredictor(
    lookback=12,                  # N√∫mero de lags
    iterations=500,               # N√∫mero de √°rvores
    learning_rate=0.03,           # Taxa de aprendizado
    depth=6,                      # Profundidade das √°rvores
    l2_leaf_reg=3.0,              # Regulariza√ß√£o L2
    random_strength=1.0,          # For√ßa da aleatoriedade
    bagging_temperature=1.0       # Temperatura do bagging
)

model.fit(train_data)
predictions = model.predict(steps=12)
```

**Quando usar**:
- Dados com rela√ß√µes n√£o-lineares complexas
- Quando XGBoost ou LightGBM est√£o sofrendo overfitting
- Dados com features categ√≥ricas
- Produ√ß√£o (treinamento r√°pido)

**Performance**: MAPE t√≠pico: 3-10%

**Hiperpar√¢metros chave**:
- `iterations`: 300-1000 (mais = melhor, mas mais lento)
- `learning_rate`: 0.01-0.1 (‚Üì para evitar overfitting)
- `depth`: 4-10 (‚Üë = mais complexo, risco de overfit)

---

### 4. **LightGBM** (`LightGBMPredictor`)

**Descri√ß√£o**: Gradient boosting ultra-r√°pido da Microsoft.

**Vantagens**:
- ‚úÖ **Extremamente r√°pido** (10-100x mais que XGBoost)
- ‚úÖ Baix√≠ssimo uso de mem√≥ria
- ‚úÖ Excelente para datasets grandes (>10k pontos)
- ‚úÖ Suporta missing values nativamente
- ‚úÖ Paraleliza√ß√£o eficiente

**Uso**:
```python
from src.models import LightGBMPredictor

model = LightGBMPredictor(
    lookback=12,                  # N√∫mero de lags
    n_estimators=500,             # N√∫mero de √°rvores
    learning_rate=0.05,           # Taxa de aprendizado
    num_leaves=31,                # N√∫mero de folhas (espec√≠fico do LightGBM)
    max_depth=-1,                 # -1 = sem limite
    min_child_samples=20,         # M√≠nimo de samples nas folhas
    subsample=0.8,                # Fra√ß√£o de samples
    colsample_bytree=0.8,         # Fra√ß√£o de features
    reg_alpha=0.1,                # Regulariza√ß√£o L1
    reg_lambda=0.1                # Regulariza√ß√£o L2
)

model.fit(train_data)
predictions = model.predict(steps=12)
```

**Quando usar**:
- Datasets grandes (>10,000 pontos)
- Quando velocidade √© cr√≠tica
- Recursos computacionais limitados
- Produ√ß√£o em larga escala

**Performance**: MAPE t√≠pico: 3-10%

**Hiperpar√¢metros chave**:
- `num_leaves`: 20-50 (espec√≠fico do LightGBM, controla complexidade)
- `n_estimators`: 300-1000
- `learning_rate`: 0.01-0.1

---

## üîç Otimiza√ß√£o Autom√°tica de Hiperpar√¢metros

### **HyperparameterOptimizer**

Usa **Optuna** (Bayesian Optimization) para encontrar os melhores hiperpar√¢metros.

**Vantagens sobre Grid Search**:
- ‚úÖ **Muito mais eficiente** (10-100x menos trials)
- ‚úÖ Aprende com trials anteriores (Bayesian)
- ‚úÖ Pruning autom√°tico de trials ruins
- ‚úÖ Suporta otimiza√ß√£o paralela

**Uso B√°sico**:
```python
from src.optimization import HyperparameterOptimizer
from src.models import CatBoostPredictor

# Cria otimizador
optimizer = HyperparameterOptimizer(
    metric='mape',           # M√©trica a minimizar
    direction='minimize',    # ou 'maximize'
    n_trials=50,             # N√∫mero de trials
    verbose=True
)

# Define espa√ßo de busca
param_space = {
    'lookback': ('int', 6, 24),                    # (tipo, min, max)
    'iterations': ('int', 100, 500),
    'learning_rate': ('float', 0.01, 0.1, 'log'), # 'log' = escala logar√≠tmica
    'depth': ('int', 4, 10)
}

# Otimiza
best_params = optimizer.optimize_model(
    model_class=CatBoostPredictor,
    train_data=train_data,
    val_data=val_data,
    param_space=param_space,
    forecast_horizon=12
)

# Usa melhores par√¢metros
model = CatBoostPredictor(**best_params)
```

**Otimiza√ß√£o de M√∫ltiplos Modelos**:
```python
# Define configura√ß√µes de m√∫ltiplos modelos
model_configs = [
    {
        'class': CatBoostPredictor,
        'param_space': {
            'lookback': ('int', 6, 24),
            'iterations': ('int', 100, 500),
            'learning_rate': ('float', 0.01, 0.1, 'log')
        }
    },
    {
        'class': LightGBMPredictor,
        'param_space': {
            'lookback': ('int', 6, 24),
            'n_estimators': ('int', 100, 500),
            'learning_rate': ('float', 0.01, 0.1, 'log')
        }
    }
]

# Otimiza todos
all_best_params = optimizer.optimize_ensemble(
    model_configs=model_configs,
    train_data=train_data,
    val_data=val_data,
    forecast_horizon=12
)
```

---

### **RecursiveOptimizer**

Reotimiza hiperpar√¢metros **durante o treinamento** se a performance estagnar.

**Como Funciona**:
1. Monitora performance a cada N epis√≥dios
2. Se n√£o melhorar o suficiente, reotimiza hiperpar√¢metros
3. Atualiza modelos com novos par√¢metros
4. Continua treinamento

**Uso**:
```python
from src.optimization import HyperparameterOptimizer, RecursiveOptimizer

# Cria otimizadores
hp_optimizer = HyperparameterOptimizer(n_trials=20)

recursive_opt = RecursiveOptimizer(
    hyperparameter_optimizer=hp_optimizer,
    reoptimize_frequency=50,      # Reotimiza a cada 50 epis√≥dios
    performance_window=20,         # Janela para calcular performance
    improvement_threshold=0.05     # Reotimiza se melhoria < 5%
)

# Durante o treinamento
for episode in range(n_episodes):
    # ... treina epis√≥dio ...

    # Checa se deve reotimizar
    if recursive_opt.should_reoptimize(current_performance):
        print("üîÑ Reotimizando hiperpar√¢metros...")

        new_params = recursive_opt.reoptimize(
            model_configs=model_configs,
            train_data=recent_train_data,
            val_data=val_data
        )

        # Atualiza modelos
        for model_name, params in new_params.items():
            # Recria modelo com novos par√¢metros
            ...
```

---

## üìä Compara√ß√£o de Modelos

| Modelo | Velocidade | Precis√£o | Interpretabilidade | Quando Usar |
|--------|------------|----------|-------------------|-------------|
| **ARIMA** | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | S√©ries lineares, sazonalidade simples |
| **AutoARIMA** | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ARIMA sem saber par√¢metros |
| **Prophet** | ‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | Outliers, dados faltantes, m√∫ltiplas sazonalidades |
| **LSTM** | ‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ | Depend√™ncias longas, padr√µes complexos |
| **XGBoost** | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ | Rela√ß√µes n√£o-lineares |
| **CatBoost** | ‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ | Features categ√≥ricas, menos overfitting |
| **LightGBM** | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ | Datasets grandes, produ√ß√£o |

---

## üí° Recomenda√ß√µes de Uso

### **Para M√°xima Precis√£o**:
```python
models = [
    AutoARIMAPredictor(seasonal=True),
    ProphetPredictor(seasonality_mode='multiplicative'),
    CatBoostPredictor(iterations=500, depth=8),
    LightGBMPredictor(n_estimators=500, num_leaves=40)
]

# Otimiza hiperpar√¢metros
optimizer = HyperparameterOptimizer(n_trials=50)
# ... otimiza ...

# Usa ensemble com RL
ensemble = EnsemblePredictor(models)
# ... treina com RL ...
```

### **Para Velocidade (Produ√ß√£o)**:
```python
models = [
    LightGBMPredictor(n_estimators=200),  # Muito r√°pido
    ProphetPredictor(),                   # R√°pido e robusto
    AutoARIMAPredictor(stepwise=True)     # Busca r√°pida
]
```

### **Para Interpretabilidade**:
```python
models = [
    AutoARIMAPredictor(trace=True),       # Mostra par√¢metros
    ProphetPredictor()                    # Componentes separados
]
```

---

## üöÄ Exemplo Completo

```python
from src.models import AutoARIMAPredictor, ProphetPredictor, CatBoostPredictor, LightGBMPredictor
from src.models import EnsemblePredictor
from src.optimization import HyperparameterOptimizer
from src.agents import AdvancedRLAgent
from src.training import AdvancedRLTrainer

# 1. Cria modelos avan√ßados
models = [
    AutoARIMAPredictor(name="AutoARIMA"),
    ProphetPredictor(name="Prophet"),
    CatBoostPredictor(iterations=300, name="CatBoost"),
    LightGBMPredictor(n_estimators=300, name="LightGBM")
]

# 2. Otimiza hiperpar√¢metros
optimizer = HyperparameterOptimizer(metric='mape', n_trials=30)

model_configs = [
    {
        'class': CatBoostPredictor,
        'param_space': {
            'iterations': ('int', 100, 500),
            'learning_rate': ('float', 0.01, 0.1, 'log'),
            'depth': ('int', 4, 10)
        }
    },
    {
        'class': LightGBMPredictor,
        'param_space': {
            'n_estimators': ('int', 100, 500),
            'learning_rate': ('float', 0.01, 0.1, 'log'),
            'num_leaves': ('int', 20, 50)
        }
    }
]

best_params = optimizer.optimize_ensemble(
    model_configs, train_data, val_data, forecast_horizon=12
)

# 3. Cria modelos otimizados
optimized_models = [
    AutoARIMAPredictor(),
    ProphetPredictor(),
    CatBoostPredictor(**best_params['CatBoostPredictor']),
    LightGBMPredictor(**best_params['LightGBMPredictor'])
]

# 4. Cria ensemble
ensemble = EnsemblePredictor(optimized_models)
ensemble.fit(train_data)

# 5. Treina agente RL avan√ßado
agent = AdvancedRLAgent(
    state_dim=env.observation_space.shape[0],
    action_dim=len(optimized_models),
    use_per=True,
    use_transformer=True
)

trainer = AdvancedRLTrainer(env, agent, ensemble)
history = trainer.train(n_episodes=200)

# 6. Avalia
results = trainer.evaluate(n_episodes=10)
print(f"MAPE: {results['mape']:.2f}%")
```

---

## üì¶ Depend√™ncias

Instale as novas depend√™ncias:

```bash
pip install prophet catboost lightgbm pmdarima optuna plotly
```

Ou instale todas de uma vez:

```bash
pip install -r requirements.txt
```

---

## üéØ Pr√≥ximos Passos

1. **Execute o exemplo**: `python examples/advanced_models_example.py`
2. **Otimize seus modelos**: Use `HyperparameterOptimizer`
3. **Experimente diferentes combina√ß√µes**: Teste diferentes ensembles
4. **Use otimiza√ß√£o recursiva**: Para m√°xima performance

---

## üìö Refer√™ncias

- **AutoARIMA**: [pmdarima docs](https://alkaline-ml.com/pmdarima/)
- **Prophet**: [Facebook Prophet](https://facebook.github.io/prophet/)
- **CatBoost**: [CatBoost docs](https://catboost.ai/)
- **LightGBM**: [LightGBM docs](https://lightgbm.readthedocs.io/)
- **Optuna**: [Optuna docs](https://optuna.readthedocs.io/)

---

**Desenvolvido com ‚ù§Ô∏è para previs√£o de ciclos econ√¥micos com t√©cnicas state-of-the-art**
