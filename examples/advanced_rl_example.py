"""
Exemplo de uso do Agente RL AvanÃ§ado (NÃ­vel PhD).

Demonstra todas as tÃ©cnicas avanÃ§adas implementadas:
- Transformer-based Actor-Critic
- Multi-Head Attention
- Prioritized Experience Replay
- Noisy Networks
- Dueling Architecture
- LSTM Memory
- Ensemble Critics
- Adaptive Entropy Regularization
- Learning Rate Scheduling
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# ImportaÃ§Ãµes do framework
from src.environments.timeseries_env import TimeSeriesEnv
from src.agents.rl_agent_advanced import AdvancedRLAgent
from src.training.trainer_advanced import AdvancedRLTrainer
from src.models.ensemble_predictor import EnsemblePredictor


def generate_sample_data(n_points: int = 200) -> pd.DataFrame:
    """
    Gera dados sintÃ©ticos de sÃ©rie temporal com mÃºltiplos padrÃµes.

    Args:
        n_points: NÃºmero de pontos

    Returns:
        DataFrame com sÃ©rie temporal
    """
    dates = pd.date_range(start='2010-01-01', periods=n_points, freq='M')

    # Componentes da sÃ©rie
    trend = np.linspace(100, 150, n_points)
    seasonality = 10 * np.sin(2 * np.pi * np.arange(n_points) / 12)
    cycle = 15 * np.sin(2 * np.pi * np.arange(n_points) / 48)  # Ciclo de 4 anos
    noise = np.random.normal(0, 3, n_points)

    # SÃ©rie temporal completa
    values = trend + seasonality + cycle + noise

    # Adiciona evento extremo (crise)
    if n_points > 100:
        values[100:110] *= 0.9  # Queda de 10%

    return pd.DataFrame({
        'date': dates,
        'value': values
    })


def main():
    """
    FunÃ§Ã£o principal demonstrando o uso do agente avanÃ§ado.
    """
    print("="*80)
    print("ğŸ“ DEMONSTRAÃ‡ÃƒO: Agente RL AvanÃ§ado (NÃ­vel PhD)")
    print("="*80)
    print()

    # 1. PreparaÃ§Ã£o dos dados
    print("ğŸ“Š 1. Preparando dados...")
    data = generate_sample_data(n_points=200)
    print(f"   âœ“ Dados gerados: {len(data)} pontos")
    print(f"   âœ“ PerÃ­odo: {data['date'].min()} a {data['date'].max()}")
    print()

    # 2. ConfiguraÃ§Ã£o do ambiente
    print("ğŸ—ï¸  2. Configurando ambiente de RL...")
    env = TimeSeriesEnv(
        data=data,
        forecast_horizon=12,  # 12 meses Ã  frente
        window_size=24,       # Janela de 24 meses
        n_coefficients=10,    # 10 coeficientes para otimizar
        max_steps=50
    )
    print(f"   âœ“ Ambiente criado")
    print(f"   âœ“ EspaÃ§o de observaÃ§Ã£o: {env.observation_space.shape}")
    print(f"   âœ“ EspaÃ§o de aÃ§Ã£o: {env.action_space.shape}")
    print()

    # 3. CriaÃ§Ã£o do agente avanÃ§ado
    print("ğŸ¤– 3. Criando Agente RL AvanÃ§ado...")
    agent = AdvancedRLAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.shape[0],
        learning_rate=1e-4,
        gamma=0.99,
        gae_lambda=0.95,
        hidden_dim=512,        # Rede grande
        num_heads=8,           # 8 cabeÃ§as de atenÃ§Ã£o
        num_layers=3,          # 3 camadas de transformer
        use_per=True,          # Prioritized Experience Replay
        use_noisy=True,        # Noisy Networks
        use_lstm=True,         # LSTM para memÃ³ria
        buffer_size=100000,    # Buffer grande
        device='cpu'           # Use 'cuda' se tiver GPU
    )
    print(f"   âœ“ Agente criado com {sum(p.numel() for p in agent.policy.parameters()):,} parÃ¢metros")
    print(f"   âœ“ Arquitetura: Transformer com {agent.policy.num_heads} heads")
    print(f"   âœ“ Hidden dim: {agent.policy.hidden_dim}")
    print()

    # 4. ConfiguraÃ§Ã£o do treinamento
    print("ğŸ¯ 4. Configurando treinamento...")
    trainer = AdvancedRLTrainer(
        env=env,
        agent=agent,
        log_dir='./logs_advanced',
        checkpoint_dir='./checkpoints_advanced',
        use_curriculum=True  # Curriculum learning
    )
    print(f"   âœ“ Trainer configurado")
    print(f"   âœ“ Curriculum Learning: Ativado")
    print(f"   âœ“ Early Stopping: Ativado")
    print()

    # 5. Treinamento
    print("ğŸš€ 5. Iniciando treinamento...")
    print()

    history = trainer.train(
        n_episodes=200,        # NÃºmero de episÃ³dios (use mais em produÃ§Ã£o)
        max_steps=50,          # Passos por episÃ³dio
        eval_frequency=25,     # Avalia a cada 25 episÃ³dios
        save_frequency=50,     # Salva a cada 50 episÃ³dios
        early_stopping=True,   # Para se nÃ£o melhorar
        verbose=True
    )

    print()
    print("="*80)
    print("âœ… Treinamento concluÃ­do!")
    print("="*80)
    print()

    # 6. AvaliaÃ§Ã£o final
    print("ğŸ“ˆ 6. Avaliando agente treinado...")
    results = trainer.evaluate(n_episodes=10, deterministic=True, verbose=True)

    # 7. AnÃ¡lise de resultados
    print("ğŸ“Š 7. AnÃ¡lise de Resultados:")
    print(f"   â€¢ Melhor recompensa: {history['best_reward']:.2f}")
    print(f"   â€¢ Recompensa mÃ©dia final: {np.mean(agent.episode_rewards):.2f}")
    print(f"   â€¢ Total de gradient steps: {agent.gradient_steps:,}")
    print(f"   â€¢ Learning rate final: {agent.optimizer.param_groups[0]['lr']:.2e}")
    print(f"   â€¢ Entropy coef final: {np.exp(agent.log_entropy_coef.detach().cpu().numpy()):.3f}")
    print()

    if 'mape' in results:
        print("ğŸ“‰ MÃ©tricas de PrevisÃ£o:")
        print(f"   â€¢ MAPE: {results['mape']:.2f}%")
        print(f"   â€¢ RMSE: {results['rmse']:.4f}")
        print(f"   â€¢ MAE: {results['mae']:.4f}")
        print()

    # 8. VisualizaÃ§Ã£o
    print("ğŸ“Š 8. Gerando visualizaÃ§Ãµes...")
    trainer.plot_training_progress(save_path='./training_advanced_progress.png')
    print()

    # 9. Salva modelo final
    print("ğŸ’¾ 9. Salvando modelo final...")
    agent.save('./models/advanced_rl_final.pt')
    print("   âœ“ Modelo salvo em ./models/advanced_rl_final.pt")
    print()

    print("="*80)
    print("ğŸ‰ DemonstraÃ§Ã£o completa!")
    print("="*80)
    print()
    print("ğŸ“š TÃ©cnicas Implementadas:")
    print("   âœ“ Transformer-based Actor-Critic com Multi-Head Attention")
    print("   âœ“ Prioritized Experience Replay (PER)")
    print("   âœ“ Noisy Networks para exploraÃ§Ã£o adaptativa")
    print("   âœ“ Dueling Architecture")
    print("   âœ“ LSTM para memÃ³ria temporal")
    print("   âœ“ Ensemble de 3 Critics")
    print("   âœ“ Adaptive Entropy Regularization")
    print("   âœ“ Learning Rate Scheduling com Warmup")
    print("   âœ“ Curriculum Learning")
    print("   âœ“ Early Stopping")
    print("   âœ“ Gradient Clipping")
    print("   âœ“ Weight Decay (L2 Regularization)")
    print()
    print("ğŸ“ Este Ã© um modelo de nÃ­vel PhD com tÃ©cnicas state-of-the-art!")
    print()


if __name__ == "__main__":
    main()
